{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "from toposample import Config, data\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from figure_helper import FigureHelper\n",
    "\n",
    "\"\"\"\n",
    "Paths to relevant data. \n",
    "Relevant data is loaded from the main data set.\n",
    "\"\"\"\n",
    "cfg = Config(\"../working_dir/config/common_config.json\")\n",
    "components_fn = cfg._cfg['analyzed']['components']\n",
    "accuracy_fn = cfg._cfg['analyzed']['accuracy_hc']\n",
    "num_stimuli = cfg.stage(\"split_spikes\")[\"config\"][\"num_stimuli\"]\n",
    "num_components = cfg.stage(\"manifold_analysis\")[\"config\"][\"n_components\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Loading the data\n",
    "\"\"\"\n",
    "per_stim_read_fun = data.read_multiple_h5_datasets(dict([(\"stim{0}\".format(i),\n",
    "                                                          \"per_stimulus/stim{0}\".format(i))\n",
    "                                                         for i in range(num_stimuli)]))\n",
    "components_main = data.TopoData(components_fn, follow_link_functions={\"data_fn\": [per_stim_read_fun, True]})\n",
    "accuracy_data = data.TopoData(accuracy_fn)[\"accuracy\"]\n",
    "comp_data = components_main[\"data_fn\"]\n",
    "comp_parent = components_main[\"idv_label\"] # not used..?\n",
    "\n",
    "\"\"\"\n",
    "Additional parameterization\n",
    "\"\"\"\n",
    "stim_colors = [\"maroon\", \"red\", \"orange\", \"green\", \"teal\", \"purple\", \"blue\", \"turquoise\"]\n",
    "exemplary_tribes = {\"Figure-S2\": {\"sampling\": \"subsampled\",\n",
    "                                  \"specifier\": \"In-degree@95\",\n",
    "                                  \"index\": \"0\"},\n",
    "                    \"Figure-S3\": {\"sampling\": \"subsampled\"},\n",
    "                    \"Panel-A\": {\"sampling\": \"subsampled\",\n",
    "                                  \"specifier\": \"In-degree@95\",\n",
    "                                  \"index\": \"0\"},\n",
    "                    \"Panel-B\": {\"sampling\": \"subsampled\",\n",
    "                                  \"specifier\": \"In-degree@95\"}\n",
    "                   }\n",
    "# For Figure-2/Panel-A\n",
    "components_to_plot_b = [7, 8, 9]\n",
    "components_to_plot_a = [1, 2, 3]\n",
    "min_to_plot_variability = 0.004\n",
    "# For Figure2/Panel-B\n",
    "interesting_indices = numpy.array([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"])\n",
    "\n",
    "fhlpr = FigureHelper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Once loaded, the data can be looked up by \"sampling\", \"specifier\" and \"index\" from comp_data.\n",
    "Each data point is a dict with stimulus identity as key and the value has the shape time x components x trials.\n",
    "\n",
    "Instead we want to turn the stimulus identity and component into additional conditions that can be used to look \n",
    "up data from comp_data. To do that we use the \"unpool\" function that allows us to define additional dimensions.\n",
    "We use \"unpool\" with a custom function that splits the data by stimulus and component.\n",
    "\"\"\"\n",
    "def make_stimulus_and_component_condition(data):\n",
    "    for k, v in data.res.items():\n",
    "        for comp in range(v.shape[1]):\n",
    "            yield v[:, comp, :], {\"stimulus\": k, \"component\": comp} # time x trials\n",
    "\n",
    "comp_data.unpool(make_stimulus_and_component_condition)\n",
    "\n",
    "for cond in comp_data.conditions():\n",
    "    print(\"{0}:\\n\\t{1}\".format(cond, comp_data.labels_of(cond)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "S = fhlpr(\"Figure-S2\")\n",
    "exemplary_tribe = exemplary_tribes[\"Figure-S2\"]\n",
    "\n",
    "data_points = comp_data.filter(**exemplary_tribe)\n",
    "\n",
    "for comp in data_points.labels_of(\"component\"):\n",
    "    ax = fig.add_subplot(4, 3, comp + 1) # hard coded for 12 components =(\n",
    "    for col, stim in zip(stim_colors, data_points.labels_of(\"stimulus\")):\n",
    "        x_data = data_points.get2(stimulus=stim, component=comp).transpose()\n",
    "        mn_data = x_data.mean(axis=0)\n",
    "        sem_data = numpy.std(x_data, axis=0) / numpy.sqrt(x_data.shape[0])\n",
    "        ax.errorbar(range(len(mn_data)), mn_data, yerr=sem_data, color=col, marker='o', ms=3, lw=1.0)\n",
    "        sel_idx = numpy.random.choice(x_data.shape[0], 5, replace=False)\n",
    "        for sel_data in x_data[sel_idx]:\n",
    "            ax.plot(range(len(sel_data)), sel_data,\n",
    "                    lw=0.25, ms=2, color=col)\n",
    "    ax.set_xticks([0, 2, 4, 6, 8])\n",
    "    ax.set_xticklabels([0, 40, 80, 120, 160])\n",
    "S.save(fig, fn=\"index-{0}\".format(exemplary_tribe[\"index\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here, we evaluate the idea that the first component just tends to respond to novelty, i.e. it shoots up \n",
    "whenever there's a new stimulus, independent of stimulus identity. \n",
    "\n",
    "For evaluation we formulate a model where a component goes up in the first time step of a new stimulus to a fixed\n",
    "value and then exponentially decays. We then fit that model to the data and see what fraction of the variance it\n",
    "explains (we load that from 'component_reacts_to_novelty.py').\n",
    "\"\"\"\n",
    "from component_reacts_to_novelty import eval_reacts_to_novelty_model\n",
    "S = fhlpr(\"Figure-S3\")\n",
    "exemplary_tribe = exemplary_tribes[\"Figure-S3\"]\n",
    "\n",
    "data_points = comp_data.filter(**exemplary_tribe)\n",
    "\n",
    "def ordered_stack(x, y, out_fun=numpy.array):\n",
    "    idx = numpy.argsort(x)\n",
    "    return out_fun([y[i] for i in idx])\n",
    "\n",
    "# By pooling over all stimuli, we can evaluate to what degree a component fits the 'novelty' model\n",
    "novelty_alignment = data_points.pool(['stimulus'], func=eval_reacts_to_novelty_model)\n",
    "# Now we pool the results for all comoponents of a sample together into an array for plotting\n",
    "novelty_alignment = novelty_alignment.pool(['component'], func=ordered_stack, xy=True)\n",
    "ax = plt.gca()\n",
    "# Get results for all sampling strategies and specifications\n",
    "I = numpy.vstack(novelty_alignment.get())\n",
    "for i in I:\n",
    "    ax.plot(numpy.arange(len(i)) + 0.2*(numpy.random.rand() - 0.5), i,\n",
    "            marker='.', ls='None', color='black', ms=2)\n",
    "ax.errorbar(range(len(i)), numpy.mean(I, axis=0), yerr=numpy.std(I, axis=0) / numpy.sqrt(I.shape[0]), color='blue')\n",
    "S.save(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here, we simply plot the average trajectories of some exemplary components in 3d coordinate systems.\n",
    "We plot two selections of components and provide two views (from different angles) of each of them.\n",
    "\"\"\"\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "S = fhlpr(\"Figure-2\")\n",
    "exemplary_tribe = exemplary_tribes[\"Panel-A\"]\n",
    "\n",
    "data_points = comp_data.filter(**exemplary_tribe)\n",
    "\n",
    "def plot_selected_components(components_to_plot, two_axes):\n",
    "    def plot_func(ax):\n",
    "        for col, stim in zip(stim_colors, data_points.labels_of(\"stimulus\")):\n",
    "            x_data = [data_points.get2(stimulus=stim, component=comp)\n",
    "                      for comp in components_to_plot] # [component] x time_steps x trials\n",
    "            mn_data = numpy.vstack([_x.mean(axis=1) for _x in x_data]) # component x time_steps\n",
    "            ax.plot(numpy.hstack([0, mn_data[0]]),\n",
    "                    numpy.hstack([0, mn_data[1]]),\n",
    "                    numpy.hstack([0, mn_data[2]]),\n",
    "                    marker='o', color=col)\n",
    "            for idx, val in enumerate(mn_data.transpose()): # for each time step\n",
    "                if numpy.sqrt(numpy.sum(val ** 2)) >= min_to_plot_variability:\n",
    "                    sem_data = numpy.array([numpy.std(_x[idx, :]) / numpy.sqrt(_x.shape[1])\n",
    "                                            for _x in x_data]) # components\n",
    "                    z = numpy.zeros(2)\n",
    "                    ax.plot(mn_data[0, idx] + numpy.array([-sem_data[0], sem_data[0]]),\n",
    "                           mn_data[1, idx] + z,\n",
    "                           mn_data[2, idx] + z, color=col, lw=1)\n",
    "                    ax.plot(mn_data[0, idx] + z,\n",
    "                            mn_data[1, idx] + numpy.array([-sem_data[1], sem_data[1]]),\n",
    "                           mn_data[2, idx] + z, color=col, lw=1)\n",
    "                    ax.plot(mn_data[0, idx] + z,\n",
    "                           mn_data[1, idx] + z,\n",
    "                            mn_data[2, idx] + numpy.array([-sem_data[2], sem_data[2]]),\n",
    "                            color=col, lw=1)\n",
    "        ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\n",
    "    plot_func(two_axes[0])\n",
    "    plot_func(two_axes[1])\n",
    "    two_axes[0].view_init(elev=35, azim=120)\n",
    "    two_axes[0].view_init(elev=35, azim=-120)\n",
    "\n",
    "plot_selected_components(components_to_plot_a, [fig.add_subplot(2, 2, 1, projection='3d'),\n",
    "                                               fig.add_subplot(2, 2, 2, projection='3d')])\n",
    "plot_selected_components(components_to_plot_b, [fig.add_subplot(2, 2, 3, projection='3d'),\n",
    "                                               fig.add_subplot(2, 2, 4, projection='3d')])\n",
    "\n",
    "S.save(fig, fn=\"Panel-A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now, we collapse the time dimension by representing each component in each trial by the most 'extreme' value\n",
    "it takes, i.e. the value furthest away from 0. We then normalize those values by subtracting the mean and\n",
    "dividing by the std.\n",
    "\n",
    "Then we plot the resulting values for a number of exemplary neuron samples. \n",
    "\"\"\"\n",
    "def most_extreme_value(data):\n",
    "    idx = numpy.argmax(numpy.abs(data), axis=0)\n",
    "    return data[idx, range(len(idx))]\n",
    "\n",
    "def ordered_stack(x, y, out_fun=numpy.array):\n",
    "    idx = numpy.argsort(x)\n",
    "    return out_fun([y[i] for i in idx])\n",
    "\n",
    "\n",
    "def normalize(stim_labels, values):\n",
    "    overall_mean = numpy.hstack(values).mean()\n",
    "    overall_sd = numpy.hstack(values).std()\n",
    "    for lbl, val in zip(stim_labels, values):\n",
    "        yield (val - overall_mean) / overall_sd, {\"stimulus\": lbl}\n",
    "\n",
    "exemplary_tribe = exemplary_tribes[\"Panel-B\"]\n",
    "\n",
    "data_points = comp_data.filter(**exemplary_tribe)\n",
    "\n",
    "# Find for each component at each trial the most extreme value it takes through time (collapses the time steps)\n",
    "ex_vol = data_points.map(most_extreme_value) # shape: n_repetitions\n",
    "# Then normalize to mean / std across stimulus conditions\n",
    "ex_vol = ex_vol.transform(['stimulus'], func=normalize, xy=True)\n",
    "# Get mean of normalized value over repetitions\n",
    "ex_vol = ex_vol.map(numpy.mean) # scalar value\n",
    "# Pool the components into an array\n",
    "ex_vol = ex_vol.pool([\"component\"], func=ordered_stack, xy=True) # components\n",
    "# And pool these arrays for different stimuli into a 2d array\n",
    "ex_vol = ex_vol.pool([\"stimulus\"],\n",
    "                     func=lambda x, y: ordered_stack(x, y, out_fun=numpy.vstack), xy=True) # stim x component\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "for plt_idx, use_idx in enumerate(interesting_indices):\n",
    "    ax = fig.add_subplot(2, 3, plt_idx + 1)\n",
    "    ax.set_title(\"Sampled population #{0}\".format(use_idx))\n",
    "    img = ax.imshow(ex_vol.get2(index=use_idx), cmap=\"RdBu\")\n",
    "    img.set_clim([-1.5, 1.5])\n",
    "    plt.colorbar(mappable=img, ax=ax)\n",
    "    if plt_idx in [0, 3]:\n",
    "        ax.set_yticks(range(8))\n",
    "        ax.set_ylabel(\"Stimulus #\")\n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "    if plt_idx in [3, 4, 5]:\n",
    "        ax.set_xticks(range(12))\n",
    "        ax.set_xlabel(\"Component #\")\n",
    "    else:\n",
    "        ax.set_xticks([])\n",
    "S.save(fig, fn=\"Panel-B\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "exemplary_tribe = exemplary_tribes[\"Panel-C\"]\n",
    "\n",
    "def different_mean_for_different_stimuli_test(data_for_stims):\n",
    "    try:\n",
    "        return -numpy.log10(kruskal(*data_for_stims).pvalue)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "data_points = comp_data.filter(**exemplary_tribe) # (sampling, specifier, index, component, stimulus)\n",
    "acc_points = accuracy_data.filter(**exemplary_tribe) # (sampling, specifier, index)\n",
    "\n",
    "info_value = data_points.map(most_extreme_value) # shape: n_repetitions\n",
    "info_value = info_value.pool(['stimulus'], different_mean_for_different_stimuli_test) # scalar\n",
    "info_value = info_value.pool(['component'], func=ordered_stack, xy=True) # shape: components\n",
    "info_value = info_value.pool(['index'], func=lambda x, y: ordered_stack(x, y, out_fun=numpy.vstack),\n",
    "                            xy=True) # shape: index x components\n",
    "\n",
    "mean_acc = acc_points.map(lambda x: numpy.nanmean(x)) # scalar; (sampling, specifier, index)\n",
    "mean_acc = mean_acc.pool(['index'], func=ordered_stack, xy=True) # shape: index\n",
    "\n",
    "def plot_general_assessment(I, acc):\n",
    "    I[numpy.isinf(I)] = 500.0\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "    ax = fig.add_axes([0.15, 0.15, 0.6, 0.9])\n",
    "    plt.colorbar(ax.imshow(I))\n",
    "    ax.set_yticks(range(I.shape[0]))\n",
    "    ax.set_xticks(range(I.shape[1]))\n",
    "    ax.set_xlabel('Component #')\n",
    "    ax.set_ylabel('Sampled population #')\n",
    "\n",
    "    ax = fig.add_axes([0.8, 0.15, 0.15, 0.9])\n",
    "    ax.barh(numpy.arange(len(acc)), acc, color='grey')\n",
    "    ax.plot(numpy.mean(acc) * numpy.ones(2), [-0.5, len(acc) - 0.5], color='black', ls='--')\n",
    "    ax.set_ylim([len(acc) - 0.5, -0.5])\n",
    "    ax.set_xlim([0.125, 0.9])\n",
    "    ax.set_yticks(range(len(acc)))\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([0.125, 0.25, 0.5, 0.75])\n",
    "    ax.set_xticklabels(['chance', '25', '50', '75'], rotation='vertical')\n",
    "    return fig\n",
    "\n",
    "fig = plot_general_assessment(info_value.get2(),\n",
    "                              mean_acc.get2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topological_sampling",
   "language": "python",
   "name": "topological_sampling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
